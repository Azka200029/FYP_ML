{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pandas numpy opencv-python matplotlib scipy requests openpyxl cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, savgol_filter\n",
    "from scipy.fft import fft, fftfreq, ifft\n",
    "from scipy.signal import find_peaks, argrelmin, argrelmax\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'fv_hb_dataset.csv'\n",
    "VIDEO_PATH = 'videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_processing(frame_rgb, block_height, block_width):\n",
    "    \"\"\"\n",
    "    Process each block in the frame and calculate the average red value for each block.\n",
    "    \"\"\"\n",
    "    avg_red_values = np.zeros((10, 10), dtype=float)\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            block = frame_rgb[i*block_height:(i+1)*block_height, j*block_width:(j+1)*block_width]\n",
    "            avg_red = np.mean(block[:, :, 0])  # 0 index for the red channel\n",
    "            avg_red_values[i, j] = avg_red\n",
    "    return avg_red_values\n",
    "\n",
    "def butterworth_filter(signal, sampling_rate, lowcut, highcut, order=2):\n",
    "    \"\"\"\n",
    "    Apply Butterworth filter to the signal.\n",
    "    \"\"\"\n",
    "    nyquist_freq = 0.5 * sampling_rate\n",
    "    low = lowcut / nyquist_freq\n",
    "    high = highcut / nyquist_freq\n",
    "    b, a = butter(order, [low, high], btype='band', analog=False)\n",
    "    filtered_signal = filtfilt(b, a, signal)\n",
    "    return filtered_signal\n",
    "\n",
    "def apply_fft(signal, sampling_rate, low_freq=0.5, high_freq=5.0):\n",
    "    \"\"\"\n",
    "    Apply FFT to the signal.\n",
    "    \"\"\"\n",
    "    n = len(signal)\n",
    "    freq = fftfreq(n, d=1/sampling_rate)\n",
    "    fft_signal = fft(signal)\n",
    "\n",
    "    unwanted_indices = (freq < low_freq) | (freq > high_freq)\n",
    "    fft_signal[unwanted_indices] = 0\n",
    "    filtered_signal = ifft(fft_signal)\n",
    "    return filtered_signal.real\n",
    "\n",
    "\n",
    "\n",
    "def plot_blocks(data):\n",
    "    \"\"\"\n",
    "    Plot PPG signal for each block.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            axs[i, j].plot(data[i, j, :])\n",
    "            axs[i, j].set_title(f'Block {i}, {j}')\n",
    "            axs[i, j].set_xticks([])\n",
    "            axs[i, j].set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_first_block_avg_red(data):\n",
    "    \"\"\"\n",
    "    Extract the average red values for the first block of each frame.\n",
    "    \"\"\"\n",
    "    first_block_avg_red = []\n",
    "    for i in range(data.shape[2]):\n",
    "        avg_red = data[0, 0, i]\n",
    "        first_block_avg_red.append(avg_red)\n",
    "    return first_block_avg_red\n",
    "\n",
    "\n",
    "def plot_avg_red_values(data, title):\n",
    "    \"\"\"\n",
    "    Plot average red values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(data, label=title)\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Average Red Value' if 'Original' in title else 'Relative Intensity')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def PPG_Signal_Extraction(video_path):\n",
    "    \"\"\"\n",
    "    Extract PPG signal from the given video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # print(f'Frame count: {frame_count}, Width: {width}, Height: {height}, FPS: {fps}')\n",
    "\n",
    "    block_width = width // 10\n",
    "    block_height = height // 10\n",
    "\n",
    "    avg_red_values = np.zeros((10, 10, frame_count), dtype=float)\n",
    "\n",
    "    current_frame = 0\n",
    "    while cap.isOpened() and current_frame < frame_count:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        avg_red_values[:, :, current_frame] = block_processing(frame_rgb, block_height, block_width)\n",
    "        \n",
    "        current_frame += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    selected_frames = avg_red_values[:, :, 26:276]\n",
    "    \n",
    "\n",
    "    print(\"Raw PPG Signal\")\n",
    "    # plot_blocks(selected_frames)\n",
    "\n",
    "    Sc = np.zeros_like(selected_frames)\n",
    "    BF = np.zeros_like(selected_frames)\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            signal = selected_frames[i, j, :]\n",
    "            filtered_signal = butterworth_filter(signal, fps, 0.5, 5.0)\n",
    "            BF[i, j, :] = filtered_signal\n",
    "            fft_signal = apply_fft(filtered_signal, fps, 0.5, 5.0)\n",
    "            Sc[i, j, :] = fft_signal\n",
    "\n",
    "    SPPG = -1 * Sc\n",
    "\n",
    "    print(\"Denoised PPG Signal after Butterworth Filter\")\n",
    "    # plot_blocks(BF)\n",
    "\n",
    "    print(\"SPPG Signal after FFT and Butterworth Filter\")\n",
    "    # plot_blocks(SPPG)\n",
    "\n",
    "    first_block_avg_red = extract_first_block_avg_red(selected_frames)\n",
    "    first_block_avg_red_denoised = extract_first_block_avg_red(SPPG)\n",
    "\n",
    "    # plot_avg_red_values(first_block_avg_red, 'Original Average Red Values for the First Block of Each Frame')\n",
    "    # plot_avg_red_values(first_block_avg_red_denoised, 'Denoised Average Red Values for the First Block of Each Frame')\n",
    "\n",
    "\n",
    "    return SPPG, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_cycle_time(ppg_signal, sampling_rate):\n",
    "    # Compute the FFT of the PPG signal\n",
    "    fft_signal = fft(ppg_signal)\n",
    "    \n",
    "    # Compute the frequency spectrum and corresponding frequencies\n",
    "    n = len(ppg_signal)\n",
    "    freq = fftfreq(n, d=1/sampling_rate)\n",
    "    \n",
    "    # Find the index of the dominant frequency component\n",
    "    dominant_freq_idx = np.argmax(np.abs(fft_signal))\n",
    "    \n",
    "    # Calculate the period of the dominant frequency component\n",
    "    dominant_freq = freq[dominant_freq_idx]\n",
    "    if dominant_freq == 0:\n",
    "        # Avoid division by zero when the dominant frequency is 0 Hz\n",
    "        return np.inf\n",
    "    expected_period = 1 / dominant_freq\n",
    "    \n",
    "    # Convert period to cycle time (in seconds)\n",
    "    expected_cycle_time = expected_period\n",
    "    \n",
    "    return expected_cycle_time\n",
    "\n",
    "\n",
    "def detect_ppg_cycles_for_one_signal(sppg_signal, sampling_rate):\n",
    "    ppg_cycles = []\n",
    "\n",
    "    peaks, _ = find_peaks(sppg_signal)\n",
    "    valleys, _ = find_peaks(-sppg_signal)\n",
    "\n",
    "    # Ensure peaks and valleys are not empty\n",
    "    if len(peaks) == 0 or len(valleys) == 0:\n",
    "        return ppg_cycles\n",
    "\n",
    "    # # Plot the PPG signal\n",
    "    # plt.figure(figsize=(15,4))\n",
    "    # plt.plot(sppg_signal, label='PPG Signal')\n",
    "\n",
    "    # # Mark peaks and valleys differently\n",
    "    # plt.scatter(peaks, sppg_signal[peaks], color='red', marker='o', label='Peaks')\n",
    "    # plt.scatter(valleys, sppg_signal[valleys], color='blue', marker='x', label='Valleys')\n",
    "\n",
    "    # Mark start_point, systolic_peak, dicrotic_notch, diastolic_peak, and end_point\n",
    "    for peak_idx in range(len(peaks) - 1):\n",
    "        for valley_idx in range(len(valleys) - 1):\n",
    "            start_idx = valleys[valley_idx]\n",
    "            end_idx = valleys[valley_idx + 1]\n",
    "\n",
    "            # Check if the current peak is within the current valley\n",
    "            if start_idx < peaks[peak_idx] < end_idx:\n",
    "                try:\n",
    "                    systolic_peak = peaks[peak_idx]\n",
    "                    diastolic_peak = peaks[peak_idx + 1]\n",
    "                    start_point = start_idx\n",
    "                    end_point = valleys[valley_idx + 2]\n",
    "                    dicrotic_notch = valleys[valley_idx+1]\n",
    "\n",
    "                    # Check conditions for valid PPG cycle\n",
    "                    if (sppg_signal[systolic_peak] > sppg_signal[diastolic_peak]):\n",
    "                        if (sppg_signal[dicrotic_notch] > sppg_signal[start_point] and sppg_signal[dicrotic_notch] > sppg_signal[end_point]):\n",
    "\n",
    "                            # Calculate time elapsed for PPG cycle\n",
    "                            cycle_time = (end_point - start_point) / sampling_rate\n",
    "                            expected_cycle_time = get_expected_cycle_time(sppg_signal, sampling_rate)\n",
    "                            error_margin = 0.2 * expected_cycle_time\n",
    "                            # Check if time elapsed is within threshold\n",
    "                            if abs(cycle_time - expected_cycle_time) <= error_margin:\n",
    "                                \n",
    "                                # # Mark each systolic peak and diastolic peak with a different marker\n",
    "                                # plt.scatter(systolic_peak, sppg_signal[systolic_peak], color='green', marker='^')\n",
    "                                # plt.scatter(diastolic_peak, sppg_signal[diastolic_peak], color='pink', marker='d')\n",
    "                                # # Mark each dicrotic notch with a different marker\n",
    "                                # plt.scatter(dicrotic_notch, sppg_signal[dicrotic_notch], color='purple', marker='*')\n",
    "                                ppg_cycles.append((start_point, systolic_peak, dicrotic_notch, diastolic_peak, end_point))\n",
    "                                # # Mark starting and ending point with a different marker\n",
    "                                # plt.scatter(start_point, sppg_signal[start_point], color='black', marker='P')\n",
    "                                # plt.scatter(end_point, sppg_signal[end_point], color='orange', marker='s')\n",
    "                except IndexError:\n",
    "                    pass\n",
    "    # # Plot the legend\n",
    "    # plt.legend()\n",
    "    # plt.legend(['PPG Signal', 'Systolic Peak', 'Diastolic Peak', 'Dicrotic Notch', 'Start Point', 'End Point'])\n",
    "    # plt.xlabel('Frame')\n",
    "    # plt.ylabel('PPG Signal Value')\n",
    "    # plt.title('Detected Cycles')\n",
    "    # plt.show()\n",
    "\n",
    "    return ppg_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_three_ppg_cycles(ppg_cycles):\n",
    "    # Sort the detected PPG cycles based on descending systolic heights\n",
    "    sorted_cycles = sorted(ppg_cycles, key=lambda cycle: cycle[1], reverse=True)\n",
    "    # print(\"Sorted PPG cycles:\", sorted_cycles)\n",
    "\n",
    "    # If at least three PPG cycles are detected, select the top three based on systolic heights\n",
    "    if len(sorted_cycles) >= 3:\n",
    "        selected_cycles = sorted_cycles[:3]\n",
    "    elif len(sorted_cycles) > 0:\n",
    "        # If less than three PPG cycles are detected, replicate the PPG cycle with maximum systolic height to get three cycles\n",
    "        selected_cycles = sorted_cycles\n",
    "        while len(selected_cycles) < 3:\n",
    "            selected_cycles.append(sorted_cycles[0])\n",
    "    else:\n",
    "        # If no PPG cycles are detected, return an empty list\n",
    "        selected_cycles = []    \n",
    "    \n",
    "    return selected_cycles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ppg_cycles(sppg_signal, selected_cycles):\n",
    "    if len(selected_cycles) == 0:\n",
    "        return None\n",
    "    \n",
    "    max_length = max(end_idx - start_idx + 1 for start_idx, _, _, _, end_idx in selected_cycles)\n",
    "    merged_signal = np.zeros(max_length)\n",
    "    num_cycles = len(selected_cycles)\n",
    "    \n",
    "    for cycle in selected_cycles:\n",
    "        start_idx, systolic_peak_idx, dicrotic_notch_idx, diastolic_peak_idx, end_idx = cycle\n",
    "        cycle_signal = sppg_signal[start_idx:end_idx+1]\n",
    "        \n",
    "        # Pad cycle signal if necessary\n",
    "        if len(cycle_signal) < max_length:\n",
    "            pad_length = max_length - len(cycle_signal)\n",
    "            cycle_signal = np.pad(cycle_signal, (0, pad_length), mode='constant')\n",
    "        \n",
    "        merged_signal += cycle_signal\n",
    "    \n",
    "    # Take the average of the summed signals\n",
    "    merged_signal /= num_cycles\n",
    "    \n",
    "    return merged_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_merged_signal(merged_signal, sampling_rate):\n",
    "    if merged_signal is None:\n",
    "        print(\"No merged signal to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate time axis based on sampling rate\n",
    "    time_axis = np.arange(len(merged_signal)) / sampling_rate\n",
    "    \n",
    "    # Plot the merged signal\n",
    "    plt.figure()\n",
    "    plt.plot(time_axis, merged_signal, color='blue')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('PPG Signal Value')\n",
    "    plt.title('Merged PPG Signal')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPG_Cycle_Detection_and_Merging(SPPG, sampling_rate):\n",
    "    \"\"\"\n",
    "    Detect PPG cycles for each block, select three cycles, and merge them into a single signal for each block.\n",
    "    \"\"\"\n",
    "    all_ppg_cycles = []\n",
    "    all_selected_cycles = []\n",
    "    all_merged_signals = []\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            sppg_signal = SPPG[i, j, :]\n",
    "            ppg_cycles = detect_ppg_cycles_for_one_signal(sppg_signal, sampling_rate)\n",
    "            all_ppg_cycles.extend(ppg_cycles)\n",
    "            # print(f\"Detected PPG cycles for block {i}, {j}:\", ppg_cycles)\n",
    "\n",
    "            selected_cycles = select_three_ppg_cycles(ppg_cycles)\n",
    "            all_selected_cycles.extend(selected_cycles)\n",
    "            # print(f\"Selected PPG cycles {i}, {j}:\", selected_cycles)\n",
    "\n",
    "            merged_signal = merge_ppg_cycles(sppg_signal, selected_cycles)\n",
    "            # print(\"Merged PPG signal:\", merged_signal)\n",
    "             # visualize_merged_signal(merged_signal, sampling_rate)\n",
    "            all_merged_signals.append(merged_signal)\n",
    "\n",
    "    return all_merged_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(merged_signal, sampling_rate):\n",
    "    features = {}\n",
    "    \n",
    "    peaks = argrelmax(np.array(merged_signal))[0]\n",
    "    valleys = argrelmin(np.array(merged_signal))[0]\n",
    "\n",
    "    \n",
    "    # Ensure peaks and valleys are not empty\n",
    "    if len(peaks) <=1 or len(valleys) == 0:\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def __next_pow2(x):\n",
    "        return 1<<(x-1).bit_length()\n",
    "   \n",
    "    # Calculate first derivative\n",
    "    derivative_1 = np.diff(merged_signal, n=1) * (sampling_rate)\n",
    "    derivative_1_peaks = argrelmax(np.array(derivative_1))[0]\n",
    "    derivative_1_valleys = argrelmin(np.array(derivative_1))[0]\n",
    "    \n",
    "    # Calculate second derivative\n",
    "    derivative_2 = np.diff(merged_signal, n=2) * (sampling_rate)\n",
    "    derivative_2_peaks = argrelmax(np.array(derivative_2))[0]\n",
    "    derivative_2_valleys = argrelmin(np.array(derivative_2))[0]\n",
    "    \n",
    "    sp_mag = np.abs(np.fft.fft(merged_signal, n=__next_pow2(len(merged_signal))*16))\n",
    "    freqs = np.fft.fftfreq(len(sp_mag))\n",
    "    sp_mag_peaks = argrelmax(sp_mag)[0]\n",
    "    \n",
    "    # print (derivative_1_peaks)\n",
    "    # print (derivative_1_valleys)\n",
    "    \n",
    "    systolic_peak_height = merged_signal[peaks[0]]\n",
    "    features['systolic_peak_height'] = systolic_peak_height\n",
    "\n",
    "    diastolic_peak_height = merged_signal[peaks[1]]\n",
    "    features['diastolic_peak_height'] = diastolic_peak_height\n",
    "    \n",
    "    dicrotic_notch_height =  merged_signal[valleys[0]]\n",
    "    features['dicrotic_notch_height'] = dicrotic_notch_height\n",
    "\n",
    "    # Calculate pulse interval\n",
    "    pulse_interval = len(merged_signal) / sampling_rate\n",
    "    features['pulse_interval'] = pulse_interval\n",
    "\n",
    "    # Calculate augmentation index\n",
    "    augmentation_index = diastolic_peak_height / systolic_peak_height\n",
    "    features['augmentation_index'] = augmentation_index\n",
    "    \n",
    "    # Calculate relative augmentation index\n",
    "    relative_augmentation_index = (systolic_peak_height - diastolic_peak_height) / systolic_peak_height\n",
    "    features['relative_augmentation_index'] = relative_augmentation_index\n",
    "    \n",
    "    # Calculate ratio of z and x\n",
    "    ratio_z_x = dicrotic_notch_height / systolic_peak_height\n",
    "    features['ratio_z_x'] = ratio_z_x\n",
    "\n",
    "    # Calculate negative relative augmentation index\n",
    "    negative_relative_augmentation_index = (diastolic_peak_height - dicrotic_notch_height) / systolic_peak_height\n",
    "    features['negative_relative_augmentation_index'] = negative_relative_augmentation_index\n",
    "\n",
    "    # Calculate systolic peak time\n",
    "    systolic_peak_time = (peaks[0]+1) / sampling_rate\n",
    "    features['systolic_peak_time'] = systolic_peak_time\n",
    "    \n",
    "    # Calculate dicrotic notch time\n",
    "    dicrotic_notch_time = (valleys[0]+1) / sampling_rate\n",
    "    features['dicrotic_notch_time'] = dicrotic_notch_time\n",
    "    \n",
    "    # Calculate diastolic peak time\n",
    "    diastolic_peak_time = (peaks[1]+1) / sampling_rate\n",
    "    features['diastolic_peak_time'] = diastolic_peak_time\n",
    "    \n",
    "    # Calculate time between systolic and diastolic peaks\n",
    "    time_between_peaks = diastolic_peak_time - systolic_peak_time\n",
    "    features['time_between_peaks'] = time_between_peaks\n",
    "    \n",
    "\n",
    "    \n",
    "    # Calculate time between half systolic peak points\n",
    "    half_systolic_peak_points = max(merged_signal) / 2\n",
    "    width = 0\n",
    "    for value in merged_signal[peaks[0]::-1]:\n",
    "        if value >= half_systolic_peak_points:\n",
    "            width += 1\n",
    "        else:\n",
    "            break\n",
    "    for value in merged_signal[peaks[0]+1:]:\n",
    "        if value >= half_systolic_peak_points:\n",
    "            width += 1\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    time_between_half_systolic_peak_points= width / sampling_rate\n",
    "    features['time_between_half_systolic_peak_points']=time_between_half_systolic_peak_points\n",
    "    \n",
    "  \n",
    "    # Inflection point area ratio\n",
    "    inflection_point_area_ratio=sum(merged_signal[:peaks[0]]) / sum(merged_signal[peaks[0]:])\n",
    "    features['inflection_point_area_ratio'] = inflection_point_area_ratio\n",
    "    \n",
    "    # Systolic peak rising slope\n",
    "    systolic_peak_rising_slope=(systolic_peak_time / systolic_peak_height)\n",
    "    features['systolic_peak_rising_slope']=systolic_peak_rising_slope\n",
    "    \n",
    "    # Diastolic peak falling slope\n",
    "    diastolic_peak_falling_slope=(diastolic_peak_height / pulse_interval-diastolic_peak_time)\n",
    "    features['diastolic_peak_falling_slope']=diastolic_peak_falling_slope\n",
    "    \n",
    "    # Ratio of t1 and pulse interval time (tpi)\n",
    "    t1_tpi_ratio = systolic_peak_time / pulse_interval\n",
    "    features['t1_tpi_ratio'] = t1_tpi_ratio\n",
    "    \n",
    "    # Ratio of t2 and pulse interval time (tpi)\n",
    "    t2_tpi_ratio = dicrotic_notch_time/ pulse_interval\n",
    "    features['t2_tpi_ratio'] = t2_tpi_ratio\n",
    "    \n",
    "    # Ratio of t3 and pulse interval time (tpi)\n",
    "    t3_tpi_ratio = diastolic_peak_time / pulse_interval\n",
    "    features['t3_tpi_ratio'] = t3_tpi_ratio\n",
    "    \n",
    "    # Ratio of deltaT and pulse interval time (tpi)\n",
    "    deltaT_tpi_ratio = time_between_peaks / pulse_interval\n",
    "    features['deltaT_tpi_ratio'] = deltaT_tpi_ratio\n",
    "    \n",
    "    # Interval time from first PPG cycle start point (l1) in first derivative of PPF (Sf) to first maxima (a1) of Sf \n",
    "    # t_a1\n",
    "    interval_time_from_l1_to_a1 = (derivative_1_peaks[0]) / (sampling_rate)\n",
    "    features['interval_time_from_l1_to_a1'] = interval_time_from_l1_to_a1\n",
    "    \n",
    "    # Interval time from point (l1) to first minima of first PPG cycle (b1) in the Sf\n",
    "    # t_b1\n",
    "    interval_time_from_l1_to_b1 = (derivative_1_valleys[0]) / (sampling_rate)\n",
    "    features['interval_time_from_l1_to_b1'] = interval_time_from_l1_to_b1\n",
    "    \n",
    "    # Interval time from point (l1) to second maxima of the first PPG cycle (e1) in the Sf\n",
    "    # t_e1\n",
    "    interval_time_from_l1_to_e1 = (derivative_1_peaks[1]) / (sampling_rate)\n",
    "    features['interval_time_from_l1_to_e1']=interval_time_from_l1_to_e1\n",
    "    \n",
    "    # Interval time from point (l1) to second minima of the first PPG cycle (f1) in the Sf\n",
    "    # t_f1\n",
    "    if len(derivative_1_valleys) >= 2:\n",
    "        interval_time_from_l1_to_f1 = (derivative_1_valleys[1]) / (sampling_rate)\n",
    "        features['interval_time_from_l1_to_f1'] = interval_time_from_l1_to_f1\n",
    "    else:\n",
    "        # Handle the case when derivative_1_valleys does not have enough elements\n",
    "        interval_time_from_l1_to_f1 = 0\n",
    "        features['interval_time_from_l1_to_f1'] = interval_time_from_l1_to_f1\n",
    "\n",
    "    # Ratio of first minima (b2) and first maxima (a2) in the second derivative of PPG signal (Sf2)\n",
    "    # b2/a2\n",
    "    a_2 = derivative_2[derivative_2_peaks[0]]\n",
    "    b_2 = derivative_2[derivative_2_valleys[0]]\n",
    "    features['b2_a2_ratio'] = b_2 / a_2\t\n",
    "    \n",
    "    # Ratio of second maxima (e2) in Sf2 and a2\n",
    "    # e2/a2\n",
    "    if len(derivative_2_peaks) >= 2:\n",
    "        e_2 = derivative_2[derivative_2_peaks[1]]\n",
    "        features['e2_a2_ratio'] = e_2 / a_2\n",
    "    else:\n",
    "        # Handle the case when derivative_2_peaks does not have enough elements\n",
    "        e_2 = 0\n",
    "        features['e2_a2_ratio'] = e_2\n",
    "        \n",
    "    # Ratio of (b2+e2) and a2\n",
    "    if b_2 is not None and e_2 is not None:\n",
    "        b2_e2_a2_ratio = (b_2 + e_2) / a_2\n",
    "        features['b2_e2_a2_ratio'] = b2_e2_a2_ratio \n",
    "    else:\n",
    "        b2_e2_a2_ratio = 0 # Or any other appropriate value\n",
    "        features['b2_e2_a2_ratio'] = b2_e2_a2_ratio\n",
    "\n",
    "    # Interval time from the second PPG cycle start point (l2) in second derivative of PPG to a2,\n",
    "    interval_time_from_l2_to_a2=(derivative_2_peaks[0]) / (sampling_rate)\n",
    "    features['interval_time_from_l2_to_a2'] = interval_time_from_l2_to_a2\n",
    "    \n",
    "    # Interval time from point l2 ti b2\n",
    "    interval_time_from_l2_to_b2=(derivative_2_valleys[0]) / (sampling_rate)\n",
    "    features['interval_time_from_l2_to_b2'] = interval_time_from_l2_to_b2\n",
    "    \n",
    "    \n",
    "    # Ratio of ta1 and tpi\n",
    "    ta1_tpi_ratio = interval_time_from_l1_to_a1 / pulse_interval\n",
    "    features['ta1_tpi_ratio'] = ta1_tpi_ratio\n",
    "    \n",
    "      # Ratio of tb1 and tpi\n",
    "    tb1_tpi_ratio = interval_time_from_l1_to_b1 / pulse_interval\n",
    "    features['tb1_tpi_ratio'] = tb1_tpi_ratio\n",
    "    \n",
    "    \n",
    "    # Ratio of te1 and tpi\n",
    "    te1_tpi_ratio = interval_time_from_l1_to_e1 / pulse_interval\n",
    "    features['te1_tpi_ratio'] = te1_tpi_ratio\n",
    "    \n",
    "    # Ratio of time interval of l1 (tl1) and tpi\n",
    "    # tf1/tpi\n",
    "    if interval_time_from_l1_to_f1 is not None:\n",
    "        tf1_tpi_ratio = interval_time_from_l1_to_f1 / pulse_interval\n",
    "        features['tf1_tpi_ratio'] = tf1_tpi_ratio\n",
    "    else:\n",
    "        # Handle the case when interval_time_from_l1_to_f1 is None\n",
    "        tf1_tpi_ratio = 0\n",
    "        features['tf1_tpi_ratio'] = tf1_tpi_ratio\n",
    "        \n",
    "    #Ratio of ta2 and tpi\n",
    "    ta2_tpi_ratio = interval_time_from_l2_to_a2 / pulse_interval\n",
    "    features['ta2_tpi_ratio'] = ta2_tpi_ratio\n",
    "    \n",
    "    #Ratio of tb2 and tpi\n",
    "    tb2_tpi_ratio = interval_time_from_l2_to_b2 / pulse_interval\n",
    "    features['tb2_tpi_ratio'] = tb2_tpi_ratio\n",
    "    \n",
    "    # Ratio of ta1+ta2 and pulse interval (tpi), ta1+ta2/tpi\n",
    "    ta1_ta1_tpi_ratio = (interval_time_from_l1_to_a1 + interval_time_from_l2_to_a2) / pulse_interval\n",
    "    features['ta1_ta1_tpi_ratio'] = ta1_ta1_tpi_ratio\n",
    "    \n",
    "    # Ratio of (tb1+tb2) and pulse interval (tpi)\n",
    "    tb1_tb2_tpi_ratio = (interval_time_from_l1_to_b1 + interval_time_from_l2_to_b2) / pulse_interval\n",
    "    features['tb1_tb2_tpi_ratio'] = tb1_tb2_tpi_ratio\n",
    "    \n",
    "    # Ratio of (te1+t2) and pulse interval (tpi)\n",
    "    te1_te2_tpi_ratio = (interval_time_from_l1_to_e1 + dicrotic_notch_time) / pulse_interval\n",
    "    features['te1_te2_tpi_ratio'] = te1_te2_tpi_ratio\n",
    "    \n",
    "    # Ratio of tl1+t3 and pulse interval (tpi)\n",
    "    if interval_time_from_l1_to_f1 is not None:\n",
    "        tf1_t3_tpi_ratio = (interval_time_from_l1_to_f1 + diastolic_peak_time) / pulse_interval\n",
    "        features['tf1_t3_tpi_ratio'] = tf1_t3_tpi_ratio\n",
    "    else:\n",
    "        # Handle the case when interval_time_from_l1_to_f1 is None\n",
    "        tf1_t3_tpi_ratio = 0\n",
    "        features['tf1_t3_tpi_ratio'] = tf1_t3_tpi_ratio\n",
    "        \n",
    "        \n",
    "    # Fundamental component frequency obtained from Fast Fourier Transformation (FFT)\n",
    "    f_base = freqs[sp_mag_peaks[0]] * sampling_rate\n",
    "    features['f_base'] = f_base\n",
    "    \n",
    "    # Fundamental component magnitude from FFT, |sbase|\n",
    "    sp_mag_base = sp_mag[sp_mag_peaks[0]] / len(merged_signal)\n",
    "    features['sp_mag_base'] = sp_mag_base\n",
    "    \n",
    "    # Second component frequency obtained from FFT. Such that, fbase<f2nd\n",
    "    if len(sp_mag_peaks) >= 2:\n",
    "        f_second = freqs[sp_mag_peaks[1]] * sampling_rate\n",
    "        features['f_second'] = f_second\n",
    "    else:\n",
    "        # Handle the case when sp_mag_peaks does not have enough elements\n",
    "        f_second = 0\n",
    "        features['f_second'] = f_second\n",
    "\n",
    "\n",
    "    # Second component magnitude from FFT\n",
    "    if len(sp_mag_peaks) >= 2:\n",
    "        sp_mag_second = sp_mag[sp_mag_peaks[1]] / len(merged_signal)\n",
    "        features['sp_mag_second'] = sp_mag_second\n",
    "    else:\n",
    "        # Handle the case when sp_mag_peaks does not have enough elements\n",
    "        sp_mag_second = 0\n",
    "        features['sp_mag_second'] = sp_mag_second\n",
    "        \n",
    "        \n",
    "    # Third component frequency obtained from FFT. Such that, fbase<f2nd<f3rd\n",
    "    if len(sp_mag_peaks) >= 3:\n",
    "        f_third = freqs[sp_mag_peaks[2]] * sampling_rate\n",
    "        features['f_third'] = f_third\n",
    "    else:\n",
    "        # Handle the case when sp_mag_peaks does not have enough elements\n",
    "        f_third = None\n",
    "        features['f_third'] = f_third\n",
    "    \n",
    "    \n",
    "    # Third component magnitude acquired from FFT\n",
    "    sp_mag_third = sp_mag[sp_mag_peaks[2]] / len(merged_signal)\n",
    "    features['sp_mag_third'] = sp_mag_third\n",
    "        \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Extraction(all_merged_signals, sampling_rate):\n",
    "    \"\"\"\n",
    "    Extract features from merged signals and return a DataFrame.\n",
    "    \"\"\"\n",
    "    # Remove None values\n",
    "    filtered_array = [cycle for cycle in all_merged_signals if cycle is not None]\n",
    "    all_merged_signals = filtered_array\n",
    "\n",
    "    all_extracted_features = []\n",
    "    for merged_signal in all_merged_signals:\n",
    "        extracted_features = extract_features(merged_signal, sampling_rate)\n",
    "        all_extracted_features.append(extracted_features)\n",
    "\n",
    "    # Filter out empty dictionaries\n",
    "    non_empty_dicts = [d for d in all_extracted_features if d]\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(non_empty_dicts)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interval_bins(df):\n",
    "    \"\"\"\n",
    "    Calculate interval bins for each column based on minimum and maximum values.\n",
    "    \"\"\"\n",
    "    min_max_values = df.agg(['min', 'max'])\n",
    "    interval_bins = {col: pd.cut(df[col], bins=np.linspace(min_val, max_val, num=11), include_lowest=True).cat.categories for col, (min_val, max_val) in min_max_values.items()}\n",
    "    return interval_bins\n",
    "\n",
    "def bin_data(df, interval_bins):\n",
    "    \"\"\"\n",
    "    Bin data based on interval bins.\n",
    "    \"\"\"\n",
    "    df_binned = pd.DataFrame({col: pd.cut(df[col], bins=interval_bins[col]) for col in df.columns})\n",
    "    return df_binned\n",
    "\n",
    "def extract_adjacent_bins(interval_bins, largest_bin):\n",
    "    \"\"\"\n",
    "    Extract adjacent bins for the largest bin.\n",
    "    \"\"\"\n",
    "    largest_interval_left = largest_bin.left\n",
    "    largest_interval_right = largest_bin.right\n",
    "    adjacent_right = None\n",
    "    adjacent_left = None\n",
    "    for interval in interval_bins:\n",
    "        interval_left = interval.left\n",
    "        interval_right = interval.right\n",
    "        if interval_left == largest_interval_right:\n",
    "            adjacent_right = interval\n",
    "        if interval_right == largest_interval_left:\n",
    "            adjacent_left = interval\n",
    "    return adjacent_left, adjacent_right\n",
    "\n",
    "def calculate_averaged_value(largest_bin_values, adjacent_left_values, adjacent_right_values, total_count, col):\n",
    "    \"\"\"\n",
    "    Calculate the averaged value.\n",
    "    \"\"\"\n",
    "    largest_bin_sum = largest_bin_values[col].sum()\n",
    "    adjacent_left_sum = adjacent_left_values[col].sum() if not adjacent_left_values.empty else 0\n",
    "    adjacent_right_sum = adjacent_right_values[col].sum() if not adjacent_right_values.empty else 0\n",
    "    averaged_value = (largest_bin_sum + adjacent_left_sum + adjacent_right_sum) / total_count if total_count != 0 else 0\n",
    "    return averaged_value\n",
    "\n",
    "def plot_histogram(df):\n",
    "    \"\"\"\n",
    "    Plot histogram for each feature.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        plt.figure()\n",
    "        plt.hist(df[col], bins=10, edgecolor='black')\n",
    "        plt.title(col)\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Selection_and_Vector_Generation(df):\n",
    "    \"\"\"\n",
    "    Perform feature selection and generate a feature vector.\n",
    "    \"\"\"\n",
    "    # Determine interval bins\n",
    "    interval_bins = calculate_interval_bins(df)\n",
    "\n",
    "    # Bin data\n",
    "    df_binned = bin_data(df, interval_bins)\n",
    "\n",
    "    # Plot histogram\n",
    "    # plot_histogram(df)\n",
    "\n",
    "    f_cap_vec = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Get the counts of values in each bin\n",
    "        bin_counts = df_binned[col].value_counts()\n",
    "\n",
    "        # Find the largest bin\n",
    "        largest_bin = bin_counts.idxmax()\n",
    "\n",
    "        # Extract adjacent bins\n",
    "        adjacent_left, adjacent_right = extract_adjacent_bins(interval_bins[col], largest_bin)\n",
    "\n",
    "        # Get the indices of the largest bin and its adjacent bins\n",
    "        largest_bin_idx = bin_counts.index.get_loc(largest_bin)\n",
    "        left_adjacent_bin_idx = bin_counts.index.get_loc(adjacent_left) if adjacent_left is not None else None\n",
    "        right_adjacent_bin_idx = bin_counts.index.get_loc(adjacent_right) if adjacent_right is not None else None\n",
    "\n",
    "        # Extract the bins and their counts\n",
    "        largest_bin_count = bin_counts.iloc[largest_bin_idx]\n",
    "        left_adjacent_bin_count = bin_counts.iloc[left_adjacent_bin_idx] if left_adjacent_bin_idx is not None else 0\n",
    "        right_adjacent_bin_count = bin_counts.iloc[right_adjacent_bin_idx] if right_adjacent_bin_idx is not None else 0\n",
    "        # print(\"largest_bin_count:\", largest_bin_count)\n",
    "        # print(\"left_adjacent_bin_count:\", left_adjacent_bin_count)\n",
    "        # print(\"right_adjacent_bin_count:\", right_adjacent_bin_count)\n",
    "        total_count = largest_bin_count + left_adjacent_bin_count + right_adjacent_bin_count\n",
    "\n",
    "        # Extract values from the largest bin and adjacent bins\n",
    "        largest_bin_values = df[df[col].apply(lambda x: x in largest_bin)]\n",
    "        adjacent_left_values = df[df[col].apply(lambda x: x in adjacent_left)] if adjacent_left is not None else pd.DataFrame()\n",
    "        adjacent_right_values = df[df[col].apply(lambda x: x in adjacent_right)] if adjacent_right is not None else pd.DataFrame()\n",
    "\n",
    "        # print(\"adjacent_left_values:\", adjacent_left_values)\n",
    "        # print(\"adjacent_right_values:\", adjacent_right_values)\n",
    "\n",
    "        # Calculate the averaged value\n",
    "        averaged_value = calculate_averaged_value(largest_bin_values, adjacent_left_values, adjacent_right_values, total_count, col)\n",
    "\n",
    "        # Update the final feature vector f^ for the subject\n",
    "        f_cap_vec[col] = averaged_value\n",
    "\n",
    "    return f_cap_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# video_path = 'videos/100_waheeda_1.mp4'\n",
    "\n",
    "# # Extract PPG signal from the given video\n",
    "# SPPG, sampling_rate = PPG_Signal_Extraction(video_path)\n",
    "\n",
    "# # Detect PPG cycles for each block, select three cycles, and merge them into a single sign for each block\n",
    "# all_merged_signals = PPG_Cycle_Detection_and_Merging(SPPG, sampling_rate)\n",
    "\n",
    "\n",
    "# # Extract features from merged signals and return a DataFrame\n",
    "# Feature_matrix = Feature_Extraction(all_merged_signals, sampling_rate)\n",
    "\n",
    "\n",
    "# # Perform feature selection and generate a feature vector\n",
    "# final_feature_vector = Feature_Selection_and_Vector_Generation(Feature_matrix)\n",
    "# print(\"Final feature vector:\")\n",
    "# print(final_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n",
      "Raw PPG Signal\n",
      "Denoised PPG Signal after Butterworth Filter\n",
      "SPPG Signal after FFT and Butterworth Filter\n"
     ]
    }
   ],
   "source": [
    "def process_videos(folder_path, markup_csv_path):\n",
    "    \"\"\"\n",
    "    Process videos in the folder and match them with the corresponding HB values.\n",
    "    \"\"\"\n",
    "\n",
    "    # List all files in the videos folder\n",
    "    video_files = os.listdir(VIDEO_PATH)\n",
    "    video_names=[]\n",
    "    for video in video_files:\n",
    "        video_names.append(video)\n",
    "        \n",
    "\n",
    "    # Load markup CSV\n",
    "    markup_df = pd.read_csv(markup_csv_path)\n",
    "    dataset = markup_df[markup_df[\"FileName\"].isin(video_names)]\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Iterate through videos in the folder\n",
    "    Final_Feature_Matrix = []\n",
    "    for video_file in os.listdir(folder_path):\n",
    "        if video_file.endswith((\".mp4\", \".avi\", \".mkv\", \".mov\")):\n",
    "            video_path = os.path.join(folder_path, video_file)\n",
    "\n",
    "    \n",
    "\n",
    "            # Extract PPG signal from the given video\n",
    "            SPPG, sampling_rate = PPG_Signal_Extraction(video_path)\n",
    "\n",
    "            # Detect PPG cycles for each block, select three cycles, and merge them into a single sign for each block\n",
    "            all_merged_signals = PPG_Cycle_Detection_and_Merging(SPPG, sampling_rate)\n",
    "\n",
    "            # Extract features from merged signals and return a DataFrame\n",
    "            Feature_matrix = Feature_Extraction(all_merged_signals, sampling_rate)\n",
    "\n",
    "            # Perform feature selection and generate a feature vector\n",
    "            final_feature_vector = Feature_Selection_and_Vector_Generation(Feature_matrix)\n",
    "            # print(\"Final feature vector:\")\n",
    "            # print(final_feature_vector)\n",
    "\n",
    "            # Match with corresponding HB value from markup CSV\n",
    "            markup_row = dataset[dataset['FileName'] == video_file]\n",
    "            if not markup_row.empty:\n",
    "                hb_value = markup_row.iloc[0]['HB']\n",
    "                Final_Feature_Matrix.append((final_feature_vector, hb_value))\n",
    "    \n",
    "    return Final_Feature_Matrix\n",
    "\n",
    "Final_Feature_Matrix = process_videos(VIDEO_PATH, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'systolic_peak_height': 0.03587660759915237, ...</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'systolic_peak_height': 0.25979103331113995, ...</td>\n",
       "      <td>12.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'systolic_peak_height': 0.19852035675448557, ...</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'systolic_peak_height': 0.30843968564777974, ...</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'systolic_peak_height': 0.3751770410833512, '...</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'systolic_peak_height': 0.7327315947182126, '...</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'systolic_peak_height': 0.9486087597899997, '...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'systolic_peak_height': 0.36060207917926157, ...</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'systolic_peak_height': 0.9481917344611472, '...</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'systolic_peak_height': 0.30649864138253513, ...</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0     1\n",
       "0  {'systolic_peak_height': 0.03587660759915237, ...  14.4\n",
       "1  {'systolic_peak_height': 0.25979103331113995, ...  12.3\n",
       "2  {'systolic_peak_height': 0.19852035675448557, ...  11.4\n",
       "3  {'systolic_peak_height': 0.30843968564777974, ...  11.5\n",
       "4  {'systolic_peak_height': 0.3751770410833512, '...  10.7\n",
       "5  {'systolic_peak_height': 0.7327315947182126, '...  10.6\n",
       "6  {'systolic_peak_height': 0.9486087597899997, '...  12.0\n",
       "7  {'systolic_peak_height': 0.36060207917926157, ...  10.3\n",
       "8  {'systolic_peak_height': 0.9481917344611472, '...  13.9\n",
       "9  {'systolic_peak_height': 0.30649864138253513, ...  12.6"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Final_Feature_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
